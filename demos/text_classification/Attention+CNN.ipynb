{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Embedding, Flatten\n",
    "from tensorflow.keras.backend import flatten, batch_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/ChnSentiCorp_htl_all.csv'\n",
    "pd_all = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate balanced corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_positive = pd_all[pd_all.label==1]\n",
    "pd_negative = pd_all[pd_all.label==0]\n",
    "\n",
    "def get_balance_corpus(corpus_size, corpus_pos, corpus_neg):\n",
    "    sample_size = corpus_size // 2\n",
    "    pd_corpus_balance = pd.concat([corpus_pos.sample(sample_size, replace=corpus_pos.shape[0]<sample_size), \\\n",
    "                                   corpus_neg.sample(sample_size, replace=corpus_neg.shape[0]<sample_size)])\n",
    "    \n",
    "    print('Comments number(overall)：%d' % pd_corpus_balance.shape[0])\n",
    "    print('Comments number(positive)：%d' % pd_corpus_balance[pd_corpus_balance.label==1].shape[0])\n",
    "    print('Comments number(negative)：%d' % pd_corpus_balance[pd_corpus_balance.label==0].shape[0])    \n",
    "    \n",
    "    return pd_corpus_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments number(overall)：4000\n",
      "Comments number(positive)：2000\n",
      "Comments number(negative)：2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6934</th>\n",
       "      <td>0</td>\n",
       "      <td>位置有点篇，房间不是很隔音，不过100多的房价也就这个样了。免费注册网站导航宾馆索引服务说明...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>1</td>\n",
       "      <td>总体感觉还不错。房间很干净、简洁。网上所披露的位于海河最美的一段，特意要了6层的房间，可是却...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>0</td>\n",
       "      <td>房价较贵，位置也比较偏，房屋隔音效果较差。屋外说话，里面的清清楚楚。不是每间房间都有宽带，我...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>1</td>\n",
       "      <td>老酒店翻新，总体来说还可以，性价比比较高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>1</td>\n",
       "      <td>在该区是最好的酒店，但离市区的酒店有一些差距。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1</td>\n",
       "      <td>两个月里面已经住了三次了,当然好啦,交通也很方便哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4792</th>\n",
       "      <td>1</td>\n",
       "      <td>上个礼拜刚去过...环境还是不错的.饭菜也很可口,价格略贵.房间不错,很古朴,很安静,在那休...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>0</td>\n",
       "      <td>一分钟一块钱的网费，贵的惊人。三个门，只有中间的转门开放，其他的门都封了(不知原因)，没有门...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>0</td>\n",
       "      <td>酒店服的服度不太有貌,入住的房比陋,房.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6487</th>\n",
       "      <td>0</td>\n",
       "      <td>好小的门面，没有电梯，房间也不是很一致！最郁闷的是我不知道我们公司跟他们有合约，他就让我的客...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             review\n",
       "6934      0  位置有点篇，房间不是很隔音，不过100多的房价也就这个样了。免费注册网站导航宾馆索引服务说明...\n",
       "5022      1  总体感觉还不错。房间很干净、简洁。网上所披露的位于海河最美的一段，特意要了6层的房间，可是却...\n",
       "5417      0  房价较贵，位置也比较偏，房屋隔音效果较差。屋外说话，里面的清清楚楚。不是每间房间都有宽带，我...\n",
       "1089      1                               老酒店翻新，总体来说还可以，性价比比较高\n",
       "777       1                            在该区是最好的酒店，但离市区的酒店有一些差距。\n",
       "530       1                          两个月里面已经住了三次了,当然好啦,交通也很方便哈\n",
       "4792      1  上个礼拜刚去过...环境还是不错的.饭菜也很可口,价格略贵.房间不错,很古朴,很安静,在那休...\n",
       "7240      0  一分钟一块钱的网费，贵的惊人。三个门，只有中间的转门开放，其他的门都封了(不知原因)，没有门...\n",
       "5841      0                               酒店服的服度不太有貌,入住的房比陋,房.\n",
       "6487      0  好小的门面，没有电梯，房间也不是很一致！最郁闷的是我不知道我们公司跟他们有合约，他就让我的客..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChnSentiCorp_htl_ba_2000 = get_balance_corpus(4000, pd_positive, pd_negative)\n",
    "\n",
    "ChnSentiCorp_htl_ba_2000.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChnSentiCorp_htl_ba_2000\n",
    "dataset.columns = ['label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from string import punctuation\n",
    "from zhon import hanzi\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the punctuations (both English and Chinese)\n",
    "def remove_punctuation(input_string):\n",
    "    punc = punctuation + hanzi.punctuation\n",
    "    output = re.sub(r'[{}]+'.format(punc), '', input_string)\n",
    "    return output\n",
    "\n",
    "def jieba_cut(sentence):\n",
    "    sent_seg = jieba.cut(sentence)\n",
    "    sent_new = ' '.join(sent_seg)\n",
    "    sent_new = remove_punctuation(sent_new)\n",
    "    sent_strip = sent_new.strip()\n",
    "    return sent_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "I0925 13:54:28.133116 12088 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "I0925 13:54:28.137073 12088 __init__.py:131] Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.993 seconds.\n",
      "I0925 13:54:29.129253 12088 __init__.py:163] Loading model cost 0.993 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "I0925 13:54:29.131248 12088 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'任由 着  你 来 了  你 走 了'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"任由着，你来了，你走了\"\n",
    "jieba_cut(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(axis=0,how='any') #drop all rows that have any NaN values\n",
    "clean_sent = ['']*len(dataset)\n",
    "for i in range(len(dataset['sentence'])):\n",
    "    sent = dataset['sentence'].tolist()[i]\n",
    "    try:\n",
    "        clean_sent[i] = jieba_cut(sent)\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence to index vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def build_dictionary(text):\n",
    "    \"\"\"\n",
    "    Build a dictionary\n",
    "    :param text: list of sentences (pre-tokens)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    wordcount = {}\n",
    "    for cc in text:\n",
    "        words = cc.split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 0\n",
    "            wordcount[w] += 1\n",
    "\n",
    "    sorted_words = sorted(list(wordcount.keys()), key=lambda x: wordcount[x], reverse=True)\n",
    "\n",
    "    worddict = OrderedDict()\n",
    "    worddict[\"<PAD>\"] = 0\n",
    "    worddict[\"<UNK>\"] = 1\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        worddict[word] = idx+2\n",
    "\n",
    "#     print(\"vocab.py: build_dictionary: wordict\", worddict, type(worddict))\n",
    "\n",
    "    return worddict, wordcount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary, _ = build_dictionary(clean_sent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(list_sent, dictionary):\n",
    "    index = [np.nan]*len(list_sent)\n",
    "    for i in range(len(list_sent)):\n",
    "        sent = list_sent[i]\n",
    "        list_words = jieba_cut(sent).split()\n",
    "        list_index = [0]*len(list_words)\n",
    "        for j in range(len(list_words)):\n",
    "            list_index[j] = dictionary.get(list_words[j], 1)\n",
    "        index[i] = np.array(list_index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   71,    16,    21,   315,    34,  7025,     9,  7025, 10200,\n",
       "           5,   100,    21,     2,   138,   422,    34,    14, 10201,\n",
       "           2,   124,  1275,    70,    33,    68,   200,    34,    49,\n",
       "         383,     2,   104,   188,     3,    42,   195,    94,    61,\n",
       "         148,    45,    65,    46,   979,  1898,     2,   744,     4,\n",
       "          12,  2197,     7,     5,   443,  1497,  1275,     2,   380,\n",
       "        1898,   212,  1275,   239,     9,   325,    64,   285,  1662,\n",
       "          19,   229,  7026,   181,  1898,   336,   592,  1898,   223,\n",
       "          23,    19,     4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_index = encode_sent(dataset['sentence'].tolist(), word_dictionary)\n",
    "all_sentence_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "all_data = keras.preprocessing.sequence.pad_sequences(all_sentence_index,\n",
    "                                                        value=word_dictionary[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data, \n",
    "                                                     dataset['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_label to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = list(y_train.value_counts().index)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_labels)\n",
    "num_labels = len(y_labels)\n",
    "y_train = to_categorical(y_train.map(lambda x: le.transform([x])[0]), num_labels)\n",
    "y_test = to_categorical(y_test.map(lambda x: le.transform([x])[0]), num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([637, 683]), array([683, 637]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(padding_len = 256, dict_len=10 , output_dim=100):\n",
    "    \n",
    "    inputs = Input(shape=(padding_len,),name=\"input_layer\")\n",
    "    \n",
    "    emb = Embedding(dict_len, output_dim, input_length=None, name=\"embedding_layer\")(inputs)\n",
    "\n",
    "    # ATTENTION PART STARTS HERE\n",
    "    attention_probs = Dense(output_dim, activation='softmax', name='attention_vec')(emb)\n",
    "    attention_mul = Multiply()([emb, attention_probs]) \n",
    "    # ATTENTION PART FINISHES HERE\n",
    "\n",
    "    attention_mul = Dense(64)(attention_mul)\n",
    "    flat = Flatten()(attention_mul)\n",
    "    print(flat.shape)\n",
    "    output = Dense(2, activation='softmax')(flat)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16384)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 256, 100)     2177600     input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 256, 100)     10100       embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 256, 100)     0           embedding_layer[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256, 64)      6464        multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 16384)        0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 2)            32770       flatten_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,226,934\n",
      "Trainable params: 2,226,934\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dict_len = len(word_dictionary)\n",
    "m = build_model(dict_len=dict_len)\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21776"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2679 samples, validate on 1320 samples\n",
      "Epoch 1/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 0.6919 - acc: 0.5222 - val_loss: 0.6875 - val_acc: 0.5674\n",
      "Epoch 2/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.6761 - acc: 0.5607 - val_loss: 0.6708 - val_acc: 0.5818\n",
      "Epoch 3/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.6427 - acc: 0.6166 - val_loss: 0.6819 - val_acc: 0.5795\n",
      "Epoch 4/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.5223 - acc: 0.7555 - val_loss: 0.4479 - val_acc: 0.8053\n",
      "Epoch 5/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.3073 - acc: 0.8850 - val_loss: 0.3392 - val_acc: 0.8568\n",
      "Epoch 6/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.2045 - acc: 0.9235 - val_loss: 0.3126 - val_acc: 0.8682\n",
      "Epoch 7/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.1458 - acc: 0.9492 - val_loss: 0.3167 - val_acc: 0.8765\n",
      "Epoch 8/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 0.1052 - acc: 0.9660 - val_loss: 0.3183 - val_acc: 0.8773\n",
      "Epoch 9/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0799 - acc: 0.9784 - val_loss: 0.3342 - val_acc: 0.8727\n",
      "Epoch 10/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0589 - acc: 0.9843 - val_loss: 0.3715 - val_acc: 0.8667\n",
      "Epoch 11/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0502 - acc: 0.9858 - val_loss: 0.3806 - val_acc: 0.8644\n",
      "Epoch 12/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0359 - acc: 0.9922 - val_loss: 0.4105 - val_acc: 0.8583\n",
      "Epoch 13/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0276 - acc: 0.9951 - val_loss: 0.4485 - val_acc: 0.8545\n",
      "Epoch 14/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0220 - acc: 0.9959 - val_loss: 0.4578 - val_acc: 0.8614\n",
      "Epoch 15/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0176 - acc: 0.9966 - val_loss: 0.4837 - val_acc: 0.8530\n",
      "Epoch 16/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0125 - acc: 0.9985 - val_loss: 0.5177 - val_acc: 0.8485\n",
      "Epoch 17/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 0.0100 - acc: 0.9981 - val_loss: 0.5445 - val_acc: 0.8515\n",
      "Epoch 18/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0075 - acc: 0.9985 - val_loss: 0.5757 - val_acc: 0.8432\n",
      "Epoch 19/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0053 - acc: 0.9993 - val_loss: 0.6039 - val_acc: 0.8417\n",
      "Epoch 20/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.6342 - val_acc: 0.8402\n",
      "Epoch 21/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.6608 - val_acc: 0.8417\n",
      "Epoch 22/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.6814 - val_acc: 0.8394\n",
      "Epoch 23/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.7038 - val_acc: 0.8394\n",
      "Epoch 24/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7247 - val_acc: 0.8409\n",
      "Epoch 25/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7381 - val_acc: 0.8379\n",
      "Epoch 26/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.7534 - val_acc: 0.8364\n",
      "Epoch 27/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 8.4968e-04 - acc: 1.0000 - val_loss: 0.7689 - val_acc: 0.8348\n",
      "Epoch 28/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 7.3448e-04 - acc: 1.0000 - val_loss: 0.7819 - val_acc: 0.8341\n",
      "Epoch 29/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 6.3332e-04 - acc: 1.0000 - val_loss: 0.7949 - val_acc: 0.8341\n",
      "Epoch 30/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 5.5705e-04 - acc: 1.0000 - val_loss: 0.8092 - val_acc: 0.8333\n",
      "Epoch 31/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.8385e-04 - acc: 1.0000 - val_loss: 0.8188 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.3828e-04 - acc: 1.0000 - val_loss: 0.8294 - val_acc: 0.8326\n",
      "Epoch 33/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.1233e-04 - acc: 1.0000 - val_loss: 0.8419 - val_acc: 0.8333\n",
      "Epoch 34/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.6826e-04 - acc: 1.0000 - val_loss: 0.8495 - val_acc: 0.8341\n",
      "Epoch 35/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.2256e-04 - acc: 1.0000 - val_loss: 0.8587 - val_acc: 0.8326\n",
      "Epoch 36/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.0063e-04 - acc: 1.0000 - val_loss: 0.8676 - val_acc: 0.8318\n",
      "Epoch 37/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.7104e-04 - acc: 1.0000 - val_loss: 0.8762 - val_acc: 0.8318\n",
      "Epoch 38/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 2.5168e-04 - acc: 1.0000 - val_loss: 0.8843 - val_acc: 0.8311\n",
      "Epoch 39/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.2732e-04 - acc: 1.0000 - val_loss: 0.8928 - val_acc: 0.8318\n",
      "Epoch 40/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.0900e-04 - acc: 1.0000 - val_loss: 0.9000 - val_acc: 0.8318\n",
      "Epoch 41/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.9578e-04 - acc: 1.0000 - val_loss: 0.9076 - val_acc: 0.8295\n",
      "Epoch 42/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 1.8371e-04 - acc: 1.0000 - val_loss: 0.9154 - val_acc: 0.8318\n",
      "Epoch 43/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.7047e-04 - acc: 1.0000 - val_loss: 0.9212 - val_acc: 0.8326\n",
      "Epoch 44/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 1.5761e-04 - acc: 1.0000 - val_loss: 0.9288 - val_acc: 0.8333\n",
      "Epoch 45/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.4858e-04 - acc: 1.0000 - val_loss: 0.9354 - val_acc: 0.8311\n",
      "Epoch 46/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 1.3537e-04 - acc: 1.0000 - val_loss: 0.9418 - val_acc: 0.8311\n",
      "Epoch 47/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.2761e-04 - acc: 1.0000 - val_loss: 0.9473 - val_acc: 0.8318\n",
      "Epoch 48/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.1971e-04 - acc: 1.0000 - val_loss: 0.9523 - val_acc: 0.8311\n",
      "Epoch 49/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.1193e-04 - acc: 1.0000 - val_loss: 0.9583 - val_acc: 0.8326\n",
      "Epoch 50/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.0534e-04 - acc: 1.0000 - val_loss: 0.9643 - val_acc: 0.8311\n",
      "Epoch 51/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 9.8360e-05 - acc: 1.0000 - val_loss: 0.9701 - val_acc: 0.8303\n",
      "Epoch 52/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 9.3431e-05 - acc: 1.0000 - val_loss: 0.9751 - val_acc: 0.8326\n",
      "Epoch 53/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 8.7560e-05 - acc: 1.0000 - val_loss: 0.9808 - val_acc: 0.8303\n",
      "Epoch 54/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 8.3389e-05 - acc: 1.0000 - val_loss: 0.9855 - val_acc: 0.8318\n",
      "Epoch 55/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 7.8848e-05 - acc: 1.0000 - val_loss: 0.9910 - val_acc: 0.8326\n",
      "Epoch 56/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 7.4660e-05 - acc: 1.0000 - val_loss: 0.9959 - val_acc: 0.8318\n",
      "Epoch 57/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 7.1308e-05 - acc: 1.0000 - val_loss: 1.0011 - val_acc: 0.8318\n",
      "Epoch 58/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 6.6869e-05 - acc: 1.0000 - val_loss: 1.0060 - val_acc: 0.8318\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 6.4249e-05 - acc: 1.0000 - val_loss: 1.0102 - val_acc: 0.8318\n",
      "Epoch 60/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 6.0543e-05 - acc: 1.0000 - val_loss: 1.0149 - val_acc: 0.8326\n",
      "Epoch 61/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 5.7494e-05 - acc: 1.0000 - val_loss: 1.0195 - val_acc: 0.8318\n",
      "Epoch 62/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 5.4831e-05 - acc: 1.0000 - val_loss: 1.0239 - val_acc: 0.8326\n",
      "Epoch 63/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 5.2717e-05 - acc: 1.0000 - val_loss: 1.0280 - val_acc: 0.8326\n",
      "Epoch 64/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 5.0609e-05 - acc: 1.0000 - val_loss: 1.0324 - val_acc: 0.8318\n",
      "Epoch 65/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.7748e-05 - acc: 1.0000 - val_loss: 1.0364 - val_acc: 0.8326\n",
      "Epoch 66/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.6052e-05 - acc: 1.0000 - val_loss: 1.0411 - val_acc: 0.8318\n",
      "Epoch 67/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.3935e-05 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.8303\n",
      "Epoch 68/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 4.1479e-05 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.8311\n",
      "Epoch 69/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.9840e-05 - acc: 1.0000 - val_loss: 1.0531 - val_acc: 0.8318\n",
      "Epoch 70/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.8385e-05 - acc: 1.0000 - val_loss: 1.0572 - val_acc: 0.8295\n",
      "Epoch 71/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.6812e-05 - acc: 1.0000 - val_loss: 1.0613 - val_acc: 0.8295\n",
      "Epoch 72/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.4933e-05 - acc: 1.0000 - val_loss: 1.0650 - val_acc: 0.8295\n",
      "Epoch 73/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.3625e-05 - acc: 1.0000 - val_loss: 1.0690 - val_acc: 0.8288\n",
      "Epoch 74/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.2216e-05 - acc: 1.0000 - val_loss: 1.0727 - val_acc: 0.8288\n",
      "Epoch 75/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 3.0912e-05 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.8295\n",
      "Epoch 76/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.9826e-05 - acc: 1.0000 - val_loss: 1.0802 - val_acc: 0.8295\n",
      "Epoch 77/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.8593e-05 - acc: 1.0000 - val_loss: 1.0836 - val_acc: 0.8273\n",
      "Epoch 78/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.7499e-05 - acc: 1.0000 - val_loss: 1.0870 - val_acc: 0.8273\n",
      "Epoch 79/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.6417e-05 - acc: 1.0000 - val_loss: 1.0909 - val_acc: 0.8280\n",
      "Epoch 80/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.5495e-05 - acc: 1.0000 - val_loss: 1.0942 - val_acc: 0.8280\n",
      "Epoch 81/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 2.4366e-05 - acc: 1.0000 - val_loss: 1.0974 - val_acc: 0.8273\n",
      "Epoch 82/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 2.3601e-05 - acc: 1.0000 - val_loss: 1.1010 - val_acc: 0.8273\n",
      "Epoch 83/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.2640e-05 - acc: 1.0000 - val_loss: 1.1042 - val_acc: 0.8273\n",
      "Epoch 84/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.2132e-05 - acc: 1.0000 - val_loss: 1.1076 - val_acc: 0.8265\n",
      "Epoch 85/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.1019e-05 - acc: 1.0000 - val_loss: 1.1108 - val_acc: 0.8258\n",
      "Epoch 86/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 2.0401e-05 - acc: 1.0000 - val_loss: 1.1142 - val_acc: 0.8258\n",
      "Epoch 87/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.9598e-05 - acc: 1.0000 - val_loss: 1.1175 - val_acc: 0.8258\n",
      "Epoch 88/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 1.8859e-05 - acc: 1.0000 - val_loss: 1.1207 - val_acc: 0.8258\n",
      "Epoch 89/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.8165e-05 - acc: 1.0000 - val_loss: 1.1233 - val_acc: 0.8273\n",
      "Epoch 90/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 1.7580e-05 - acc: 1.0000 - val_loss: 1.1270 - val_acc: 0.8265\n",
      "Epoch 91/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 1.7092e-05 - acc: 1.0000 - val_loss: 1.1302 - val_acc: 0.8265\n",
      "Epoch 92/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.6432e-05 - acc: 1.0000 - val_loss: 1.1332 - val_acc: 0.8258\n",
      "Epoch 93/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 1.5853e-05 - acc: 1.0000 - val_loss: 1.1364 - val_acc: 0.8258\n",
      "Epoch 94/100\n",
      "2679/2679 [==============================] - 7s 2ms/sample - loss: 1.5360e-05 - acc: 1.0000 - val_loss: 1.1395 - val_acc: 0.8258\n",
      "Epoch 95/100\n",
      "2679/2679 [==============================] - 7s 3ms/sample - loss: 1.4876e-05 - acc: 1.0000 - val_loss: 1.1425 - val_acc: 0.8258\n",
      "Epoch 96/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.4338e-05 - acc: 1.0000 - val_loss: 1.1457 - val_acc: 0.8258\n",
      "Epoch 97/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.3844e-05 - acc: 1.0000 - val_loss: 1.1485 - val_acc: 0.8265\n",
      "Epoch 98/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.3352e-05 - acc: 1.0000 - val_loss: 1.1511 - val_acc: 0.8265\n",
      "Epoch 99/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.2988e-05 - acc: 1.0000 - val_loss: 1.1541 - val_acc: 0.8250\n",
      "Epoch 100/100\n",
      "2679/2679 [==============================] - 6s 2ms/sample - loss: 1.2497e-05 - acc: 1.0000 - val_loss: 1.1572 - val_acc: 0.8250\n"
     ]
    }
   ],
   "source": [
    "# m.fit([X_train], y, epochs=20, batch_size=64, validation_split=0.5)\n",
    "history1 = m.fit(X_train,y_train,epochs=100,batch_size=64,\n",
    "                 validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2679, 256)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_acc(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acc(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
